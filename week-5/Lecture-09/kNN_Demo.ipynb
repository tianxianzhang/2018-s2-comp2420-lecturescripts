{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Afzal Ahmad\n",
    "# Date: 26 March 2019\n",
    "# Used for code demonstration in COMP2420 lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, metrics\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the Iris dataset, with the first two columns (Sepal Length and Width) as predictors, and we want to predict the Iris Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "# Sepal Length, Sepal Width\n",
    "X = iris.data[:,:2]\n",
    "# Iris type (Setosa=0, Versicolour=1, Virginica=2)\n",
    "y = iris.target\n",
    "iris_np = np.column_stack((X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_np[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the purpose of the exercise, let's split this up so we have data to test on\n",
    "# first we need to shuffle the data so we have a good proportion in each split\n",
    "iris_np = np.column_stack((X, y))\n",
    "np.random.shuffle(iris_np)\n",
    "X = iris_np[:,:2]\n",
    "y = iris_np[:,-1]\n",
    "\n",
    "# now we split it\n",
    "length = X.shape[0]\n",
    "X_train = X[:int(length*0.8)]\n",
    "y_train = y[:int(length*0.8)]\n",
    "X_test = X[int(length*0.8):]\n",
    "y_test = y[int(length*0.8):]\n",
    "\n",
    "# looking at the shapes to make sure we got it right\n",
    "print(\"training:\", X_train.shape, y_train.shape)\n",
    "print(\"testing:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_len = X[:,0]\n",
    "sep_wid = X[:,1]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "colors = ['green', 'red', 'blue']\n",
    "handles = []\n",
    "for i in range(3):\n",
    "    handle = plt.scatter(sep_len[y==i], sep_wid[y==i], c=colors[i], alpha=0.5, s=200)\n",
    "    handles.append(handle)\n",
    "\n",
    "plt.ylabel(\"Sepal Length\",fontsize=18)\n",
    "plt.xlabel(\"Sepal Width\",fontsize=18)\n",
    "plt.title('Iris Type by Sepal data', fontsize=18)\n",
    "\n",
    "plt.legend(handles, (\"0 (Setosa)\", \"1 (Versicolour)\", \"2 (Virginica)\"), fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1. Why are the points arranged in a grid-like fashion?\n",
    "2. Based on the distribution in the scatter-plot, which class might be the easiest to predict using sepal info, and why?\n",
    "3. Which class(es) might be the hardest to predict, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much identical to the example you saw in the lecture, just to show you how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighbour:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        # simply just remembers all the training data, complexity: O(1)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # same algorithm as seen in the lecture, implements k-Nearest Neighbour for k=1\n",
    "        num_test = X.shape[0]\n",
    "        \n",
    "        # output type matching input type\n",
    "        Ypred = np.zeros(num_test, dtype=self.y.dtype)\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            # finds the Euclidean distance between the point to predict and all trained points\n",
    "            distances = np.sum(np.square(self.X - X[i,:]), axis=1)\n",
    "            \n",
    "            # gets the index with the min distance (i.e. the closest point) for each point to preict\n",
    "            min_index = np.argmin(distances)\n",
    "            \n",
    "            # gets the class of the closest point\n",
    "            Ypred[i] = self.y[min_index]\n",
    "            \n",
    "        return Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up an instance of the NearestNeighbour class\n",
    "nn = NearestNeighbour\n",
    "\n",
    "# training the data\n",
    "NearestNeighbour.train(nn, X_train, y_train)\n",
    "\n",
    "# testing the data\n",
    "predictions = NearestNeighbour.predict(nn, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the data visually so we can see at a glance whether we're on the right track. Then we'll use a \"confusion matrix\", which just compares the actual class with the class we predicted - great for showing us where we went wrong, and if we're biased towards one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare visually\n",
    "print(predictions)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the confusion matrix lets us see how many we got right, and where we went wrong\n",
    "metrics.confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "our confusion matrix comes out as:\n",
    "\n",
    "   ___0____1____2_\n",
    " 0 |  6    0    0\n",
    " 1 |  0    9    4\n",
    " 2 |  0    5    2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we guessed Class 0 correctly 10/10 times, Class 1 correctly 9/13 times, and Class 2 correctly 2/7 times.<br>\n",
    "But we also thought 5 of the Class 2s were Class 1, and we thought 4 of the Class 1s were Class 2!<br><br>\n",
    "Now let's try out for k-Nearest Neighbour, and see if we can improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNearestNeighbour:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        # simply just remembers all the training data, complexity: O(1)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def predict(self, X, k):\n",
    "        # same algorithm as seen in the lecture, implements k-Nearest Neighbour for k=1\n",
    "        num_test = X.shape[0]\n",
    "        \n",
    "        # output type matching input type\n",
    "        Ypred = np.zeros(num_test, dtype=self.y.dtype)\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            # finds the Euclidean distance between the point to predict and all trained points\n",
    "            distances = np.sum(np.square(self.X - X[i,:]), axis=1)\n",
    "            \n",
    "            # gets the index with the min distance (i.e. the closest point) for each point to preict\n",
    "            min_index = np.argsort(distances)[:k]\n",
    "            \n",
    "            # gets the class of the closest point\n",
    "            k_classes = self.y[min_index]\n",
    "            Ypred[i] = stats.mode(k_classes).mode\n",
    "            \n",
    "        return Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up an instance of the NearestNeighbour class\n",
    "knn = kNearestNeighbour\n",
    "\n",
    "# training the data\n",
    "kNearestNeighbour.train(knn, X_train, y_train)\n",
    "\n",
    "# testing the data\n",
    "predictions = kNearestNeighbour.predict(knn, X_test, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare visually\n",
    "print(predictions)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the confusion matrix lets us see how many we got right, and where we went wrong\n",
    "metrics.confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "our confusion matrix comes out as:\n",
    "\n",
    "   ___0____1____2_\n",
    " 0 | 10    0    0\n",
    " 1 |  0    9    4\n",
    " 2 |  0    1    6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we guessed Class 0 correctly 10/10 times, Class 1 correctly 9/13 times, and Class 2 correctly 6/7 times.<br>\n",
    "But we also thought 1 of the Class 2s were Class 1, and we thought 4 of the Class 1s were Class 2. This is a little better than k=1 Nearest Neighbours though!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
